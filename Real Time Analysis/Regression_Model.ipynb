{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "\n",
    "This is the notebook for predicting wining side by player performance, namely KDA and minion killed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "import joblib\n",
    "from pandas import DataFrame as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleanData.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "\n",
    "The below codes tell us percentages of matches are won by team 1. Herem team 1 won less matches than team 2. And if we blindly choose team 2 to be the winner for all matches, this tells us the \"accuracy\" for this non-sense method and serves as the baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1win = 0\n",
    "for idx, x in df['win'].iteritems():\n",
    "    if(x==1.0):\n",
    "        t1win+=1\n",
    "print(t1win)\n",
    "print((1733658-t1win)/1733658)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns.tolist()\n",
    "print(cols)\n",
    "\n",
    "#col concerned:\n",
    "#kills, death, assists, totminionskilled, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teamRoleList = df['team_role'].unique().tolist()\n",
    "print(teamRoleList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignRoleWithNum(x):\n",
    "    return {\n",
    "        '1 - MID': 100,\n",
    "        '2 - MID': 100,\n",
    "        '1 - JUNGLE': 200,\n",
    "        '2 - JUNGLE': 200,\n",
    "        '1 - TOP': 300,\n",
    "        '2 - TOP': 300,\n",
    "        '1 - DUO_CARRY': 400,\n",
    "        '2 - DUO_CARRY': 400,\n",
    "        '1 - DUO_SUPPORT': 400,\n",
    "        '2 - DUO_SUPPORT': 400,\n",
    "        '1 - BOT': 400,\n",
    "        '2 - BOT': 400\n",
    "    }[x] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above is to assign roles with values so that we would encode the features into regression.\n",
    "\n",
    "Value of 1, 10, 100, 1000 is tried.\n",
    "Turns out that 100 is the best choice (but only 1% accuracy increase, so not much). My educational guess behind this is that kill/death/assists is in range of 0-10. Duration is usually a 4 digits value. Having 100, distinguish this team role feature from others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['role'] = df['team_role'].apply(assignRoleWithNum)\n",
    "dataset = df[['win','kills','deaths','assists','totminionskilled','duration']]\n",
    "dataset = dataset.dropna() #drop all nan rows\n",
    "dataset = dataset.take(np.random.permutation(len(dataset)))#randomize rows\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset, test_size = 0.1)\n",
    "print('train:', train.shape, 'test:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_feed(dataset):\n",
    "    team_data = dataset.iloc[:,1:] #exclude first column which is win\n",
    "    winners = dataset['win']\n",
    "    return team_data, winners \n",
    "\n",
    "trainX, trainY = get_data_feed(train)\n",
    "testX, testY = get_data_feed(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_Model = LogisticRegression()\n",
    "LR_Model.fit(trainX, trainY)\n",
    "\n",
    "prediction = LR_Model.predict(testX)\n",
    "\n",
    "\n",
    "print (\"Accuracy : \", accuracy_score(testY, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\"kills\", \"deaths\", \"assists\", \"totminionskilled\", \"duration\"]\n",
    "vals = [[1, 2, 3, 4, 5]]\n",
    "params = df(vals, columns = variables)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "rta",
   "display_name": "RTA",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}